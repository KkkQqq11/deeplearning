{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:05:47.571523Z",
     "start_time": "2024-03-25T11:05:46.948552Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ATTN\n",
    "1、除以d_k是因为q和k相乘后方差变为d_k\n",
    "2、softmax将scores转换为概率分布"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a75aa1ed2159b1e5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)  \n",
    "\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "\n",
    "    return torch.matmul(scores, value), scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:05:47.575426Z",
     "start_time": "2024-03-25T11:05:47.572358Z"
    }
   },
   "id": "fc40ec076ed069c3",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Muti-Head ATTN\n",
    "1、在张量操作之后保存内存连续性，以满足内存访问效率和后续网络层的要求\n",
    "2、"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a171a3693de507d1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = d_model // n_head\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        q = self.q(x).view(bs, -1, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.k(x).view(bs, -1, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.v(x).view(bs, -1, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        out = attention(q, k, v, dropout=self.dropout).permute(0, 2, 1, 3).contiguous().flatten(2)\n",
    "        \n",
    "        return self.fc_out(out)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:05:47.594540Z",
     "start_time": "2024-03-25T11:05:47.575909Z"
    }
   },
   "id": "f0b6649acc942b2b",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FFN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16a18245ec806cc7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, forward_expansion):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_model*forward_expansion)\n",
    "        self.w2 = nn.Linear(d_model*forward_expansion, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2((F.relu(self.w1(x))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:05:47.609091Z",
     "start_time": "2024-03-25T11:05:47.595090Z"
    }
   },
   "id": "7459e116798b0781",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9952c1129c1be684"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n           0.0000e+00,  1.0000e+00],\n         [ 8.4147e-01,  5.4030e-01,  7.9194e-01,  ...,  1.0000e+00,\n           1.0941e-05,  1.0000e+00],\n         [ 9.0930e-01, -4.1615e-01,  9.6711e-01,  ...,  1.0000e+00,\n           2.1882e-05,  1.0000e+00],\n         ...,\n         [-8.9797e-01, -4.4006e-01,  1.7700e-01,  ...,  9.9993e-01,\n           1.0908e-02,  9.9994e-01],\n         [-8.5547e-01,  5.1785e-01,  8.8749e-01,  ...,  9.9993e-01,\n           1.0919e-02,  9.9994e-01],\n         [-2.6461e-02,  9.9965e-01,  9.0684e-01,  ...,  9.9993e-01,\n           1.0930e-02,  9.9994e-01]]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000): \n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(100000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + nn.Parameter(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return x\n",
    "    \n",
    "model = PositionEmbedding(256)\n",
    "model.pe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:05:47.625737Z",
     "start_time": "2024-03-25T11:05:47.609963Z"
    }
   },
   "id": "6b313a8625783b28",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e26c4fcad67a477"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, head, forward_expansion, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(embed_size, head)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = FeedForwardLayer(embed_size, forward_expansion)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        # ipdb.set_trace()\n",
    "        attention =  self.attn(query, key, value, mask)\n",
    "        \n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:05:47.629838Z",
     "start_time": "2024-03-25T11:05:47.626386Z"
    }
   },
   "id": "59445377bf0f0b3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_size, \n",
    "        num_layers, \n",
    "        heads, \n",
    "        forward_expansion, \n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, forward_expansion, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # ipdb.set_trace()\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, mask)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:05:47.645044Z",
     "start_time": "2024-03-25T11:05:47.630597Z"
    }
   },
   "id": "7d014f7386755d3f",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84ff7582a2ef0119"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout=0.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.attn = MultiHeadAttention(embed_size, heads, dropout)\n",
    "        self.transformer = TransformerBlock(embed_size, heads, forward_expansion, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attn = self.attn(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm1(attn + x))\n",
    "        out = self.attn(query, value, key, src_mask)\n",
    "        out = self.norm2(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:05:47.655350Z",
     "start_time": "2024-03-25T11:05:47.645876Z"
    }
   },
   "id": "e51b57ecc8443237",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "            \n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_out, src_mask, trg_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_out, encoder_out, src_mask, trg_mask)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:05:47.670060Z",
     "start_time": "2024-03-25T11:05:47.656088Z"
    }
   },
   "id": "f330648abf394e14",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3732859d1e942216"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        max_length=100,  \n",
    "        device=\"cpu\",  \n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            embed_size,\n",
    "            num_encoder_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            embed_size,\n",
    "            num_decoder_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "        )\n",
    "\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1)\n",
    "        # (N, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # ipdb.set_trace()\n",
    "        N, src_seq_length = src.shape\n",
    "        N, trg_seq_length = trg.shape\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length)\n",
    "            .unsqueeze(0)\n",
    "            .expand(N, src_seq_length)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length)\n",
    "            .unsqueeze(0)\n",
    "            .expand(N, trg_seq_length)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        # encoder部分\n",
    "        x = self.dropout(\n",
    "            self.src_word_embedding(src) + self.src_position_embedding(src_positions)\n",
    "        )\n",
    "        encoder_out = self.encoder(x, src_mask)\n",
    "        # decoder部分\n",
    "        x = self.dropout(\n",
    "            self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions)\n",
    "        )\n",
    "        decoder_out = self.decoder(x, encoder_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(decoder_out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T11:05:47.680600Z",
     "start_time": "2024-03-25T11:05:47.670704Z"
    }
   },
   "id": "90591ae65c95b989",
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
