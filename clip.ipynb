{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-24T02:57:58.092822Z",
     "start_time": "2024-03-24T02:57:56.397989Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai02/miniconda3/envs/kk/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ai02/miniconda3/envs/kk/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/ai02/miniconda3/envs/kk/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained('/home/ai02/Desktop/clip-vit-base-patch32')\n",
    "processor = CLIPProcessor.from_pretrained('/home/ai02/Desktop/clip-vit-base-patch32')\n",
    "image = Image.open('/media/ai02/43633275BF4935F0/kk/datasets/RUOD/images/test/003926.jpg')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T03:17:12.389872Z",
     "start_time": "2024-03-24T03:17:11.917136Z"
    }
   },
   "id": "6ac0a5899f853364",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text = ['God', 'turtle', 'bus']\n",
    "inputs = processor(text=text, images=image, return_tensors='pt', padding=True)\n",
    "\n",
    "output = model(**inputs)\n",
    "logits = output.logits_per_image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T03:17:13.708654Z",
     "start_time": "2024-03-24T03:17:13.592924Z"
    }
   },
   "id": "63f6cf67121315b2",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.1311e-04, 9.9976e-01, 2.5429e-05]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits.softmax(dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T03:17:16.069408Z",
     "start_time": "2024-03-24T03:17:16.066624Z"
    }
   },
   "id": "e32310955f1cf08f",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CLIP'...\r\n",
      "remote: Enumerating objects: 251, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (8/8), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (8/8), done.\u001B[K\r\n",
      "remote: Total 251 (delta 3), reused 3 (delta 0), pack-reused 243\u001B[K objects:  34% (87/251), 2.04 MiB | 1016.00 KiB/sReceiving objects:  34% (87/251), 2.96 MiB | 740.00 KiB/sReceiving objects:  34% (87/251), 3.34 MiB | 378.00 KiB/sReceiving objects:  34% (87/251), 3.50 MiB | 189.00 KiB/sReceiving objects:  34% (87/251), 3.71 MiB | 116.00 KiB/sReceiving objects:  34% (87/251), 3.99 MiB | 114.00 KiB/sReceiving objects:  35% (88/251), 4.15 MiB | 135.00 KiB/sReceiving objects:  45% (114/251), 4.30 MiB | 145.00 KiB/sReceiving objects:  45% (114/251), 4.58 MiB | 146.00 KiB/sReceiving objects:  45% (114/251), 4.77 MiB | 111.00 KiB/sReceiving objects:  45% (114/251), 4.93 MiB | 104.00 KiB/sReceiving objects:  45% (114/251), 5.28 MiB | 129.00 KiB/sReceiving objects:  45% (114/251), 5.62 MiB | 158.00 KiB/sReceiving objects:  45% (114/251), 5.96 MiB | 161.00 KiB/sReceiving objects:  45% (114/251), 6.31 MiB | 169.00 KiB/sReceiving objects:  45% (114/251), 6.43 MiB | 146.00 KiB/sReceiving objects:  46% (116/251), 6.51 MiB | 121.00 KiB/sReceiving objects:  66% (166/251), 6.51 MiB | 121.00 KiB/sReceiving objects:  73% (184/251), 6.51 MiB | 121.00 KiB/sReceiving objects:  77% (194/251), 6.57 MiB | 109.00 KiB/sReceiving objects:  91% (229/251), 6.57 MiB | 109.00 KiB/sReceiving objects:  96% (241/251), 6.61 MiB | 97.00 KiB/s Receiving objects:  96% (241/251), 6.72 MiB | 66.00 KiB/sReceiving objects:  96% (241/251), 6.91 MiB | 78.00 KiB/sReceiving objects:  96% (241/251), 7.21 MiB | 105.00 KiB/sReceiving objects:  96% (241/251), 7.71 MiB | 191.00 KiB/sReceiving objects:  96% (241/251), 8.49 MiB | 309.00 KiB/s\r\n",
      "Receiving objects: 100% (251/251), 8.93 MiB | 214.00 KiB/s, done.\r\n",
      "Resolving deltas: 100% (127/127), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/openai/CLIP.git"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:35:11.880096Z",
     "start_time": "2024-04-02T06:34:27.282140Z"
    }
   },
   "id": "d3dd5c94b2b9b0d0",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting CLIP\r\n",
      "  Downloading clip-0.2.0.tar.gz (5.5 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hBuilding wheels for collected packages: CLIP\r\n",
      "  Building wheel for CLIP (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for CLIP: filename=clip-0.2.0-py3-none-any.whl size=6988 sha256=bd63c8fece2b6beb7741847d8b80fb63fde0241c4dfed25c5aa287f04521e19d\r\n",
      "  Stored in directory: /home/ai02/.cache/pip/wheels/6d/17/18/6193c6b02f9e35e3b3f0721a349b9f9f74bac11feb0f86fdd1\r\n",
      "Successfully built CLIP\r\n",
      "\u001B[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0mInstalling collected packages: CLIP\r\n",
      "Successfully installed CLIP-0.2.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install CLIP"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:37:11.245495Z",
     "start_time": "2024-04-02T06:37:08.175380Z"
    }
   },
   "id": "5d94806338e95233",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from CLIP import clip\n",
    "import torch\n",
    "\n",
    "text = ['people']\n",
    "model, _ = clip.load(\"ViT-B/32\")\n",
    "device = next(model.parameters()).device\n",
    "text_token = clip.tokenize(text).to(device)\n",
    "txt_feats = model.encode_text(text_token).to(dtype=torch.float32)\n",
    "txt_feats = txt_feats / txt_feats.norm(p=2, dim=-1, keepdim=True)\n",
    "txt_feats = txt_feats.reshape(-1, len(text), txt_feats.shape[-1]).detach()\n",
    "# self.model[-1].nc = len(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:52:59.129607Z",
     "start_time": "2024-04-02T06:52:57.592807Z"
    }
   },
   "id": "381027ae6709df79",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 512])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# txt_feats.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:42:59.319212Z",
     "start_time": "2024-04-02T06:42:59.316440Z"
    }
   },
   "id": "14690043d2973c79",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 81\u001B[0m\n\u001B[1;32m     79\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m16\u001B[39m, \u001B[38;5;241m256\u001B[39m, \u001B[38;5;241m40\u001B[39m, \u001B[38;5;241m40\u001B[39m)\n\u001B[1;32m     80\u001B[0m guide \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;241m16\u001B[39m, \u001B[38;5;241m80\u001B[39m, \u001B[38;5;241m512\u001B[39m)\n\u001B[0;32m---> 81\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mguide\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/kk/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/kk/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[6], line 68\u001B[0m, in \u001B[0;36mMaxSigmoidAttnBlock.forward\u001B[0;34m(self, x, guide)\u001B[0m\n\u001B[1;32m     65\u001B[0m embed \u001B[38;5;241m=\u001B[39m embed\u001B[38;5;241m.\u001B[39mview(bs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnh, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhc, h, w)\n\u001B[1;32m     67\u001B[0m aw \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbmchw,bnmc->bmhwn\u001B[39m\u001B[38;5;124m\"\u001B[39m, embed, guide)\n\u001B[0;32m---> 68\u001B[0m aw \u001B[38;5;241m=\u001B[39m \u001B[43maw\u001B[49m\u001B[38;5;241m.\u001B[39mmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     69\u001B[0m aw \u001B[38;5;241m=\u001B[39m aw \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhc\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n\u001B[1;32m     70\u001B[0m aw \u001B[38;5;241m=\u001B[39m aw \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias[\u001B[38;5;28;01mNone\u001B[39;00m, :, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m]\n",
      "Cell \u001B[0;32mIn[6], line 68\u001B[0m, in \u001B[0;36mMaxSigmoidAttnBlock.forward\u001B[0;34m(self, x, guide)\u001B[0m\n\u001B[1;32m     65\u001B[0m embed \u001B[38;5;241m=\u001B[39m embed\u001B[38;5;241m.\u001B[39mview(bs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnh, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhc, h, w)\n\u001B[1;32m     67\u001B[0m aw \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbmchw,bnmc->bmhwn\u001B[39m\u001B[38;5;124m\"\u001B[39m, embed, guide)\n\u001B[0;32m---> 68\u001B[0m aw \u001B[38;5;241m=\u001B[39m \u001B[43maw\u001B[49m\u001B[38;5;241m.\u001B[39mmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     69\u001B[0m aw \u001B[38;5;241m=\u001B[39m aw \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhc\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n\u001B[1;32m     70\u001B[0m aw \u001B[38;5;241m=\u001B[39m aw \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias[\u001B[38;5;28;01mNone\u001B[39;00m, :, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m]\n",
      "File \u001B[0;32m~/pycharm-2023.3.4/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:755\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    753\u001B[0m \u001B[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001B[39;00m\n\u001B[1;32m    754\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND:\n\u001B[0;32m--> 755\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    756\u001B[0m     \u001B[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001B[39;00m\n\u001B[1;32m    757\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrace_dispatch\n",
      "File \u001B[0;32m~/pycharm-2023.3.4/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:412\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pycharm-2023.3.4/plugins/python/helpers/pydev/pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1184\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pycharm-2023.3.4/plugins/python/helpers/pydev/pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1199\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def autopad(k, p=None, d=1):  # kernel, padding, dilation\n",
    "    \"\"\"Pad to 'same' shape outputs.\"\"\"\n",
    "    if d > 1:\n",
    "        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
    "    return p\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation).\"\"\"\n",
    "\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
    "        \"\"\"Initialize Conv layer with given arguments including activation.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        \"\"\"Perform transposed convolution of 2D data.\"\"\"\n",
    "        return self.act(self.conv(x))\n",
    "\n",
    "\n",
    "class MaxSigmoidAttnBlock(nn.Module):\n",
    "    \"\"\"Max Sigmoid attention block.\"\"\"\n",
    "\n",
    "    def __init__(self, c1, c2, nh=1, ec=128, gc=512, scale=False):\n",
    "        \"\"\"\n",
    "        Initializes MaxSigmoidAttnBlock with specified arguments.\n",
    "        :param nh: Number of heads\n",
    "        :param hc: Number of head channels\n",
    "        :param\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nh = nh\n",
    "        self.hc = c2 // nh\n",
    "        self.ec = Conv(c1, ec, k=1, act=False) if c1 != ec else None\n",
    "        self.gl = nn.Linear(gc, ec)\n",
    "        self.bias = nn.Parameter(torch.zeros(nh))\n",
    "        self.proj_conv = Conv(c1, c2, k=3, s=1, act=False)\n",
    "        self.scale = nn.Parameter(torch.ones(1, nh, 1, 1)) if scale else 1.0\n",
    "\n",
    "    def forward(self, x, guide):\n",
    "        \"\"\"\n",
    "        Forward process.\n",
    "        :param x: Input\n",
    "        :param guide: text_features [bs, len(nc), embeddings]\n",
    "\n",
    "        \"\"\"\n",
    "        bs, _, h, w = x.shape\n",
    "\n",
    "        guide = self.gl(guide)\n",
    "        guide = guide.view(bs, -1, self.nh, self.hc)\n",
    "        embed = self.ec(x) if self.ec is not None else x\n",
    "        embed = embed.view(bs, self.nh, self.hc, h, w)\n",
    "\n",
    "        aw = torch.einsum(\"bmchw,bnmc->bmhwn\", embed, guide)\n",
    "        aw = aw.max(dim=-1)[0]\n",
    "        aw = aw / (self.hc**0.5)\n",
    "        aw = aw + self.bias[None, :, None, None]\n",
    "        aw = aw.sigmoid() * self.scale\n",
    "\n",
    "        x = self.proj_conv(x)\n",
    "        x = x.view(bs, self.nh, -1, h, w)\n",
    "        x = x * aw.unsqueeze(2)\n",
    "        return x.view(bs, -1, h, w)\n",
    "    \n",
    "model = MaxSigmoidAttnBlock(256, 256, 8, 256)\n",
    "x = torch.randn(16, 256, 40, 40)\n",
    "guide = torch.randn(16, 80, 512)\n",
    "y = model(x, guide)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T09:11:23.560566Z",
     "start_time": "2024-04-05T12:10:24.194823Z"
    }
   },
   "id": "4105f44157468262",
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
